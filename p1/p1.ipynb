{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d27e32ba-20a7-4f5e-98e9-d30490fda143",
   "metadata": {},
   "source": [
    "# Project 1 - Text Analysis, Classification, and Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bef2100-3db5-4311-bc10-69b3c6bf0cfa",
   "metadata": {},
   "source": [
    "#### **Developer:** Mark Trombly\n",
    "#### **Course:** Artifical Intelligence Applications\n",
    "#### **Program Requirements**\n",
    "~~~\n",
    "1. Import necessary packages\n",
    "2. Review data\n",
    "3. Prepare data for analysis\n",
    "4. Filter data\n",
    "5. Display product review sentiment analysis\n",
    "6. Create prediction analysis\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcb1628-e35c-41ad-8a44-c662ac20666f",
   "metadata": {},
   "source": [
    "### Part 1: Import necessary packages/modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c46e02c-6f85-4700-b2d7-c1d35942ff11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: *After* installing NLTK (if not installed already) *must* download NLTK datasets (corpus).\n",
    "# Some important datasets: stpwords, guntenbert, framenet_v15, and others.\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "print(sys.version) # print python version\n",
    "print(os.environ['CONDA_DEFAULT_ENV']) # print conda environment\n",
    "\n",
    "import nltk # Natural Language Toolkit - language processing\n",
    "# ***AFTER initial downloading comment out!***\n",
    "# nltk.download('punkt_tab') # Sentence Tokenizer: splits text into list of sentences (must be trained before it can be used) ***AFTER initial downloading comment out!***\n",
    "# nltk.download('averaged_perceptron_tagger_eng') # contains pre-trained (Wall Street Journal) Englist [Part-of-Speech (POS)] ***AFTER initial downloading comment out!***\n",
    "\n",
    "# word tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# nltk.download('stopwords') # only if needed, then comment out\n",
    "# use to identify stop words - common words carrying little information (see below)\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# use for tagging words with their parts of speech (POS) (e.g., nouns, verbs, etc.)\n",
    "# nltk.download('averaged_perceptron_tagger') # after downloading comment out ***AFTER initial downloading comment out!***\n",
    "\n",
    "# use for sentiment analysis - analyse positive/negative emotion of text (see below)\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "# nltk.download(vader_lexicon') # after downloading comment out ***AFTER initial downloading comment out!***\n",
    "\n",
    "# required to split data into train and test sets, where feature variables are given as input in method\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# CountVectorizer() converts collection of text documents into matrix.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# classifies document based on counts it finds of multiple keywords\n",
    "from sklearn.naive_bayes import MultinomialNB # import naive bayes\n",
    "\n",
    "# used for confusion matrix in classification problems to assess errors in model\n",
    "from sklearn import metrics\n",
    "\n",
    "# determines accuracy classification score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# library for creating static, animated, and interactive visualizations in Python\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4d62e2-24e4-4a82-ac32-77d57a80f48c",
   "metadata": {},
   "source": [
    "### Part 2: Load and review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cec3cc-b631-45de-82fd-d5a192a93929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reviews into DataFrame\n",
    "# Note: pipe (|) used instead of commas, as commas occur in reviews, and # indicates indexed column\n",
    "df = pd.read_csv('GuitarReviews2out.txt', sep='|', index_cols='#')\n",
    "\n",
    "rows = df.shape[0]   # num rows\n",
    "cols = df.shape[1]   # num cols\n",
    "\n",
    "# display number of rows/cols\n",
    "print(rows)\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f34d6ed-8745-466a-8ea9-567ff7f8301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() # display first 5 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68aa942-c930-4d16-b63b-a782d88f488c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0].review # display first review (from review column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2eb16d-65ee-44ac-ab98-25cf39c01865",
   "metadata": {},
   "source": [
    "### Part 3: Prepare, tokenize, and visulize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e4afec-f85c-40e8-b5c7-3f637ea3540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all reviews from DataFrame into list for data manipulation/analysis\n",
    "allTextList = df.review.to_list()\n",
    "\n",
    "# used oly for comparison\n",
    "print(allTextList) # display list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b150d4bb-2a9e-43c2-80b8-9fba69ec2226",
   "metadata": {},
   "outputs": [],
   "source": [
    "allText = ' '.join(allTextList) # join elements of list with space\n",
    "print(allText) # Note: elements no longer separated by commas, or include single quotation marks (')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e067bb0a-7717-4b7e-82d1-319aaff361a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizers divide strings into Lists of substrings\n",
    "# resource: https://www.nltk.org/api/nltk.tokenize.html\n",
    "# example: find words and puctuation in a string\n",
    "# parse: tokenize text\n",
    "tokens = nltk.word_tokenize(allText)\n",
    "\n",
    "# print(tokens) # display all tokens\n",
    "tokens[:10] # print only first 10 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb1fa61-2820-496d-8fdf-f626dae88653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine word frequency\n",
    "# Note: FreqDist() captures number of times each outcome of experiment has occurred\n",
    "# https://www.nltk.org/api/nltk.probability.FreqDist.html\n",
    "wordFrequency = nltk.FreqDist(tokens)\n",
    "wordFrequency.plot(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0395f98-23ce-48ce-9208-29fd43b9c060",
   "metadata": {},
   "source": [
    "### Part 4: Filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320251d1-c9db-4b75-93cc-cfaafba07600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep tokens with Letters, using list comprehension\n",
    "# Note: if necessary, review list comprehensions:\n",
    "# https://www.w3schools.com/python/python_lists_comprehension.asp\n",
    "alpha_words = [token for token in tokens if token.isalpha()]\n",
    "\n",
    "alpha_words[:10] # print first 10 tokens w/letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50990d4-f626-40a2-967b-f82b9e7bb94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast alpha words into lower case, using list comprehesion\n",
    "lower_case_words = [word.lower() for word in alpha_words]\n",
    "\n",
    "lower_case_words[:10] # print first 10 tokens w/letters in lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3afe2e-7cc3-4da2-b36d-2ff151b90e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find stop words using NLTK stopwords package\n",
    "# stop words: common words carrying little information\n",
    "# explained:\n",
    "# https://www.opinosis-analytics.com/knowledge-base/stop-words-explained/\n",
    "# https://medium.com/@yashj302/stopwords-nip-python-4aa57dc492af\n",
    "# examples: \"the,\" \"is,\" \"for,\" \"where,\" \"when,\" \"to,\" \"at,\"...\n",
    "# NLTK's list of enclish stopwords: https://gist.github.com/sebleier/554280\n",
    "\n",
    "# get NLTK English stopwords\n",
    "stopWords = stopwords.words('english')\n",
    "\n",
    "type(stopWords) # print stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02f5897-890d-4875-b0b5-d81ee9d9211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stopWords) # print number of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f356e9-57f5-4a21-a950-502f5fa568eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords[:10] # display first 10 NLTK English stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81bcf3f-68f8-4fbc-8147-02497c3c975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words from lower case words\n",
    "lower_case_no_stop_words = [word for word in lower_case_words if word not in stopWords]\n",
    "\n",
    "# display first 10 tokens w/letters in lower case, and parts of speech, *after* removing NLTK Englis stop words\n",
    "lower_case_no_stop_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6c1626-0e26-4547-9ff6-9292a539b760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine lower-case words w/no stop words frequency\n",
    "wordFrequency = nltk.FreqDist(lower_case_no_stop_words)\n",
    "wordFrequency # display frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d623d6d9-6f10-4693-9754-e5ef89100fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize/plot word frequency\n",
    "wordFrequency.plot(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7b4b0a-01ce-4eac-8a36-30efdf5e89f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming: remove morphological affixes from words, leaving only word stem (algoritym for suffix stripping)\n",
    "# examples: playing = play, likes/likely/liked = like\n",
    "# use Porter Stemmer (strip word suffixes)\n",
    "porterStemmer = nltk.PorterStemmer()\n",
    "stemmed_words = [porterStemmer.stem(word) for word in lower_case_no_stop_words]\n",
    "\n",
    "# type(stemmed_words)\n",
    "stemmed_words[:10] # print first 10 stemmed words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ea0761-58b8-448c-9067-a5c21a62cefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add part of speech to each token\n",
    "# reference: https://www.nltk.org/book/ch05.html\n",
    "wordsWithTags = nltk.pos_tag(tokens)\n",
    "wordsWithTags[:10] # display first 10 tokens and their part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a1f790-d461-4592-9f76-9d0d7e4cd3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# include only nouns (tags beginning with N)\n",
    "nouns = [word for (word, tag) in wordsWithTags if tag.startswith('N')]\n",
    "nouns[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a48d4e-6759-494c-9cc1-d494517a5bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine noun frequency\n",
    "wordFrequency = nltk.FreqDist(nouns)\n",
    "wordFrequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e54268e-641c-4280-b44d-89d7f7bee622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize/plot noun frequency\n",
    "wordFrequency.plot(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafbc568-7908-4a79-bd51-8b1c4efea137",
   "metadata": {},
   "source": [
    "### Part 5: Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cde97f2-7467-4e44-88cb-721a20511e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis:\n",
    "# used to analyse positive/negative emotion of text (determine polarity of text: positive, negative, or neutral)\n",
    "# https://www.nltk.org/howto/sentiment.html\n",
    "# https://medium.com/@rslavanyageetha/vader-a-comprehensive-guide-to-sentiment-analysis-in-python-c4f1868b0d2e\n",
    "\n",
    "# initalize SentimentIntesityAnalyzer object\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885d6d0c-ceb3-458c-9f33-acc757ca8b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze first review (from review column)\n",
    "review1 = df.iloc[0].review\n",
    "review1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a85e50-d9e5-475e-9787-bb6e409a361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# polarity_scores() method: returns dictionary o sentiment scores\n",
    "# dictionary contains four key/value pairs: neg, neu, pos, and compound\n",
    "# i.e., how negative (0-1), how neutral (0-1), how ositive (0-1), as well as a compound score between -1 to 1\n",
    "# compound: composite score of overall positive or negative sentiment (e.g., 0.9646 is very positive!)\n",
    "\n",
    "# calculate polarity scores for first review\n",
    "analyzer.polarity_scores(review1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d761d4-a2ad-4ffa-8d9f-d4f2336f413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through each review using polarity_scores() function\n",
    "# display index, compound (composite) score, formated to two decimal places, and review title\n",
    "# Note: sixth review most positive, and ninth review only negative review\n",
    "compoundList = []\n",
    "for index, row in df.iterrows():\n",
    "    text = row.review\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    compound = score3s['compound']\n",
    "    print(format(index, '2d'), format(compound, '6.2f'), row.title)\n",
    "    compoundList.append(compound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0d7df9-d44b-426d-8dc0-907074ff7aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more concisely, use DataFrame apply() method\n",
    "# define function that calculates and returns compound VADER score\n",
    "def compoundScore(text):\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    return scores['compound']\n",
    "\n",
    "# apply analyzer on all reviews in DataFrame and display\n",
    "# Note: apply() method passes function as an argument, and applies it on every single value of Pandas series\n",
    "\n",
    "# apply compoundScore() function to \"review\" column in DataFrame, and create new DF column \"compound\"\n",
    "df['compound'] = df['review'].apply(compoundScore)\n",
    "df # display entire DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19775609-af70-492c-bc5d-d6f78a5ee896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or just display, index, title, and compound score\n",
    "print(df[['title', 'compound']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cbef54-c584-471e-90ec-c03cdc44817c",
   "metadata": {},
   "source": [
    "### Part 6: Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d7456f-a470-4c64-8217-332f2d527f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data file of emails into DataFrame. Note: one line per email, pipe delimited\n",
    "df = pd.read_csv('emails2.txt', usecols=['isSpam', 'Message'], sep='|')\n",
    "\n",
    "rows = df.shape[0] # num rows\n",
    "cols = df.shape[1] # num cols\n",
    "\n",
    "# display number of rows/cols\n",
    "print(rows)\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc02911-16b8-48ad-b330-3a66603472fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() # display first 5 e-mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33157d5-5cea-462f-8593-3d75c7445f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# review spam vs. nonspam emails (1=spam, 0=nonspam)\n",
    "df.isSpam.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb8f6b4-7c3a-4416-bf0a-b473aeec603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# review (part of) first email (nonspam)\n",
    "df.iloc[0].Message[:160] # display first 160 chars. of Message col."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af58a153-9e3e-48d9-a572-a4aa48c1d819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to remove nonLetters\n",
    "def remove_non_letters(text):\n",
    "    alist = [c if c.isalpha() else ' ' for c in text]\n",
    "    return ''.join(alist)\n",
    "\n",
    "# iterate over Message col. using apply() method, and create new col.\n",
    "df['NonLettersRemoved'] = df['Message'].apply(remove_non_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad363922-f617-497f-8aad-c465c9610d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display first 160 chars. of Message col.\n",
    "df.iloc[0].NonLettersRemoved[:160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f020bbdb-3202-4277-b093-d55f7c6b67c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize e-mails\n",
    "# create lambda function to tokenize filtered e-mail messages\n",
    "tokenizer = lambda text: word_tokenize(text)\n",
    "df['NonLettersRemoved'][:10] # display tokens for first 10 e-mails w/nonLetters removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3789bef7-2cdc-463c-bc7f-175549a56dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming: remove morphological affixes from words, leaving only word stem (algorithm for suffix stripping)\n",
    "# examples: playing = play, likes/likely/liked = like\n",
    "# use Porter Stemmer (strip word suffixes)\n",
    "stemmer = lambda words: [porterStemmer.stem(word) for word in words]\n",
    "df['NonLettersRemoved'] = df['NonLettersRemoved'].apply(stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a79b40f-272a-4ed8-8ce8-9bc29b68c302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display stemmed tokens for first 10 e-mails w/nonLetters removed\n",
    "df['NonLettersRemoved'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a27c05-ae22-41bd-a08a-1c39f2bb4ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Lambda function to rejoin tokenized e-mail messages\n",
    "rejoiner = lambda words: ' .join(words)\n",
    "df['NonLettersRemoved'] = df['NonLettersRemoved'].apply(rejoiner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad93dbb-49cd-4598-aa59-244d81c7a09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare initial and transformed text for first 5 (nonspam) messages\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4f7548-7a7f-40c5-a297-811e06923e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare initial and transformed text for th last 5 (spam) messages\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a955d39-09fd-4789-973f-3b0dd5d5bf8c",
   "metadata": {},
   "source": [
    "### Part 7: Predictive analysis\n",
    "\n",
    "### Definitions:\n",
    "\n",
    "  1. **Dependent variables (also called):** response, outcome/output, or target variables (respond to changes in (an)other variable(s))\n",
    "  2. **Independent variables (also called):** predictor, input, regressor, or explanatory variable(s) (predict/explain changed values of dependent variable(s))\n",
    "\n",
    "\n",
    "*Dependent* variables **(output on y-axis)** are *always* the ones *being studied*--that is, whose variation(s) is/are being modified somehow!\n",
    "\n",
    "*Independent* variables **(input on x-axis)** are *always* the ones being manipulated, to study and compare the effects on the dependent variable(s).\n",
    "\n",
    "**Note:** The designations *independent* and *dependent* variables are used to not imply \"cause and effect\" (as do \"predictor\" or \"explanatory\" terms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75af851-dad4-4a41-bf12-09e1ab2bf2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependent variable: isSpam (studied var.)\n",
    "# independent variable: NonLettersRemoved (manipulated var.)\n",
    "\n",
    "# split data into 25% \"test\" data and 75% \"train\" data\n",
    "# Note: \"generally,\" 25/75 is how data are split into text/train data sets\n",
    "\n",
    "# returns four results (all Pandas \"Series\" data type):\n",
    "# train_text and test_text: contain e-mail text\n",
    "# train_labels and test_labels: contain binary values from iSpam column\n",
    "train_text, test_text, train_labels, test_labels = train_test_split(df.NonLettersRemoved, df.isSpam, test_seze=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9874e4a-2c10-4ffe-a446-602cfc1af618",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34df7109-2fc9-4045-ac09-4206c233d290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer(): Converts collection of text documents into matrix of token counts.\n",
    "# rows represent documents, and cols represent tokens (i.e, words or n-grams).\n",
    "# counts occurrences of each token in each document.\n",
    "# Note: \"n-gram\" is collection of n successive items in a text document--may include words, numbers, symbols, and punctuation.\n",
    "# Resource: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "# use CountVectorizer() to determine word freq. for each e-mail\n",
    "# build \"bag of words\" (bow) features vecorizer and get features\n",
    "\n",
    "# min_df=1: tracks words occurring at least once\n",
    "# ngram_range=(1,1): finds single words, rather thaword combinations\n",
    "bow_vectorizer = CountVectorizer(min_df=1, ngram_range(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a56ce2-d2e2-4faf-a474-0bf83acaab23",
   "metadata": {},
   "source": [
    "#### **fit() vs. transform() vs. fit_transform() methods:**\n",
    "\n",
    "- **fit():** calculates mean and variance of each of the features in data.\n",
    "- **transform():** transforms all features using respective mean and variance\n",
    "- **fit_transform():** used on training data to scale training data, and also learn scaling parameters of data.\n",
    "\n",
    "Model built will learn mean and variance of features of training set; learned parameters then used to scale test data.\n",
    "\n",
    "**Resource:** https://towardsdatascience.com/what-and-why-behind-fit-transform-vs-transform-in-scikit-learn-78f915cf96fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368dbd50-7a84-4816-a560-74a853d4722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit_transform(): used on training data\n",
    "# counts occurrences of each wordin each e-mail\n",
    "bow_train_features = bow_vectorizer.fit_transform(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e225237a-a3c2-4930-8d9c-c5b0a14a0817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform(): used on test data\n",
    "# transform() : use same mean and variance as calculated from training data to transform test data.\n",
    "# Bottom-lin: parameters learned by model using training data helps to transform test data.\n",
    "bow_test_features = bow_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786ece5f-6e26-45d4-8d00-d10b692a6229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multinomial Naive Bayes classifier: suitable for classification with discree features (e.g., word counts for text clasification).\n",
    "# probabilistic classifier calculates probability distribution of text data\n",
    "# Multinomial Distribution: used to find probabilities in experiments, where there are more than two outcomes.\n",
    "# Resource: https://scikit-learn.org/stable/modules/generated/sklearn.nive_bayes.MultinomialNB.html\n",
    "\n",
    "model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c42a4f-2540-4f0f-a767-83309f4c38d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit(): trains machine learning model on dataset.\n",
    "# fit() method: takes in datase (typically, 2D array or matrix), and a set of labels, then fits model to data.\n",
    "# MultinomialNB fit() method: expects x and y input.\n",
    "# x: training vectors (i.e., training data)\n",
    "# y: target values (i.e., labels, targets or classes)\n",
    "\n",
    "# Note: train model using training data, then predit using new data (i.e., test data, below).\n",
    "# fit() method: determines probabilities of individual words occurring in nonspam vs spam e-mails.\n",
    "model.fit(bow_train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef19b4a-16a6-44f6-81d5-d65c880ccee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict nonspam vs spam e-mails using model and test data\n",
    "predictions = model.predict(bow_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ba58b8-1f8f-45d5-9384-2956f41d5d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of emails in test data\n",
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3f695f-6b29-48a8-9e96-f2eeca04cd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of emails in training data\n",
    "len(train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8144c4-b1b0-40fa-ba14-420f215ed0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating model's predictions:\n",
    "# Compare actual spam/nonspam e-mails with mode's prediction of spam/nonspam e-mails\n",
    "test_results = pd.DataFrame({'actual':test_labels.tolist(), 'predict':list(predictions)})\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5775089-1070-4832-97d8-e320df8e04ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display *all* rows where model was incorrect (index value indicates row position)\n",
    "# Note: only four rows!\n",
    "test_results[test_results.actual != test_results.predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3828c15b-fb35-47ef-9e36-984a8d66f58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy score for set of predicted labels against true labels\n",
    "# resource: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n",
    "accuracy_score(test_results.actual, test_results.predict)\n",
    "\n",
    "# display as percentae (note: 94% accuracy!)\n",
    "print('Accuracy {:.1%}'.format(accuracy_score(test_results.actual, test_results.predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caab983c-086c-41ff-8290-ecc8fa78778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also, can check accuracy using confusion matrix\n",
    "# creates table to assess where errors occurred in model\n",
    "# rows represent classes outcomes should have been\n",
    "# columns represent predictions made\n",
    "# table displays which predictions were wrong\n",
    "\n",
    "# import \"metrics\" to use confusion matrix function on \"actual\" and \"predicted\" values\n",
    "# rows represent actual classes that outcomes should have been\n",
    "# columns represent predictions made\n",
    "# Using table is an easy to see which predictions are wrong!\n",
    "# Generic syntax: confusion_matrix = metrics.confusion_matrix(actual, predicted)\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(test_results.actual, test_results.predict)\n",
    "\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e942007b-cb9e-46fb-80a1-e917624bb596",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0, 1])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix creates four quadrants:\n",
    "# True Negatives (TN) (Top-Left Quadrant): Prediction no, true value no\n",
    "# False Positives (FP (Top-Right Quadrant): Prediction yes, true value no\n",
    "# False Negatives (FN) (Bottom-Left Quadrant): Prediction no, true value yes\n",
    "# True Positives (TP) (Bottom-Right Quadrant): Prediction yes, true value yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3459f26-adb3-4de7-84af-b92299b57394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpretation:\n",
    "# row1: Model correctly categorized **nonspam** e-mails in 33 of 36 cases (91.7%), \"specificity\"\n",
    "# row2: Model correctly categorized **spam** e-mails in 33 out of 34 cases (97.1%), \"sensitivity\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72888009-7333-4ec0-a20c-1c99bbf80077",
   "metadata": {},
   "source": [
    "**Confusion Matrix:** Table compares predicted and actual values.\n",
    "\n",
    "||**Predicted: No (not spam)** | **Predicted: Yes (spam)** | **Total:** |\n",
    "|:---|:-----------------------:|:-------------------------:|:-----------|\n",
    "|**Actual: No  (not spam)**| TN=33 | FP=3 | 36 |\n",
    "|**Actual: Yes (spam)**| FN=1 | TP=33 | 34 |\n",
    "|**Total:** | 34 | 36 | |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7244585-5038-44fb-a499-21bd6dbc5008",
   "metadata": {},
   "source": [
    "## Related Metrics:\n",
    "\n",
    "|**Metric** | **Formula** | **Definition** |\n",
    "|:---|:-----------------------:|:-------------------------:|\n",
    "|**Accuracy**| (TP+TN)/(TP+TN+FP+FN) | Percentage of total items classified correctly |\n",
    "|**Precision**| TP/(TP+FP) | Positive predictions accuracy |\n",
    "|**Recall/Sensitivity** | TP/(TP+FN) | True positive rate (e.g., assess false positive rate) |\n",
    "|**Specificity** | TN/(TN+FP) | True negative rate (e.g., assess false negative rate) |\n",
    "|**F1 score** | 2TP/(2TP+FP+FN) | Weighted average of precision and recall/sensitivity |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973f9554-1733-4d20-b247-24b49bd7eaf8",
   "metadata": {},
   "source": [
    "**Resources:**\n",
    "https://machine-learning.paperspace.com/wiki/confusion-matrix\n",
    "https://classeval.wordpress.com/introduction/basic-evaluation-measures/\n",
    "https://poojapawani.medium.com/what-is-confusion-matrix-accuracy-sensitivity-specificity-precision-recall-1091b4723714\n",
    "https://www.w3schools.com/python/python_ml_confusion_matrix.asp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50be44bc-fbbc-4629-8df8-a30a6d2aa4ce",
   "metadata": {},
   "source": [
    "## Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d75794-d329-47e1-9621-56f0282c90bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy measures how often model is correct\n",
    "# Calculation: (True Positive + True Negative) / Total Predictions\n",
    "# Example: Accuracy = metrics.accuracy_score(actual, predicted)\n",
    "\n",
    "Accuracy = metrics.accuracy_score(test_results.actual, test_results.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dfef25-84fd-45ad-b310-fd9bcef1c688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision: Of positives predicted, what percentage is *truly* positive?\n",
    "# Note: Precision does not evaluate correctly predicted negative cases:\n",
    "\n",
    "# Calculation: True Positive / (True Positive + False Positive)\n",
    "# Example: Precision = metrics.precision_score(actual, predicted)\n",
    "\n",
    "Precision = metrics.precision_score(test_results.actual, test_results.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65932d02-151b-4076-b187-1bea2e64d675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitivity (aka Recall):\n",
    "# Of all positive cases, what percentage are *predicted* positive?\n",
    "# Measures how well model predicts something is positive.\n",
    "\n",
    "# Translation: Looks at true positives and false negatives (which are positives that have been incorrectly predicted as negative).\n",
    "\n",
    "# Calculation: True Positive / (True Positive + False Negative)\n",
    "# Example: Sensitivity recall = metrics.recall_score(actual, predicted, pos_label=0)\n",
    "\n",
    "Specificity = metrics.recall_score(test_results.actual, test_results.predict, pos_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ef4b60-3e1d-44d5-9523-72ffb325abe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F-score: \"Harmonic mean\" of precision and sensitivity.\n",
    "# Considers both false positive and false negative cases--good for imbalanced datasets.\n",
    "# Note: Score does not take into consideration True Negative values.\n",
    "\n",
    "# Calculation: 2 * ((Precision * Sensitivity) / (Prcision + Sensitivity))\n",
    "\n",
    "# Example: F1_score = metrics.f1_score(actual, predicted)\n",
    "\n",
    "F1_score = metrics.f1_score(test_results.actual, test_results.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cd3c62-e97b-418c-8048-39a3a69dadfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all calculations: print dictionary (Python dictionaries use curly braces {}), that is key:value paris\n",
    "print({\"Accuracy\":Accuracy, \"Precision\":Precision,\"Sensitivity_recall\":Sensitivity_recall,\"Specificity\":Specificity,\"F1_score\":F1_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf088a0-8fc1-498e-a31e-c740bd4f673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or, to format nicely! :)\n",
    "# my_dictionary = key:value pairs\n",
    "my_dictionary = {Accuracy\":Accuracy,\"Precision\":Precision,\"Sensitivity_recall\":Sensitivity_recall,\"Specificity\":Specificity,\"F1_score\":F1_score)\n",
    "\n",
    "# Note: \"0\" and \"1\" indicate field order--that is, key=0 and value=1\n",
    "# Note: '<' Forces field to be left-aligned, within available space (default for most objects)\n",
    "# Resource: https://docs.python.org/2/library/string.html#string-formatting\n",
    "\n",
    "print(\"\\n\".join(\"{0: <16}\\t{1:.2f}\".format(k, v) for k, v in my_dictionary.items()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
